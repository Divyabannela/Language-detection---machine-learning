# -*- coding: utf-8 -*-
"""language detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AFG0IGrwd65obHh0aJKGxXLsTCaeonWq
"""

# basic libraries
import os
import re
import pickle
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')
from tqdm import tqdm

# visualization tools
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# preprocessing tools
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder

# model building tools
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

"""Load and Generate data"""

# set directories
input_dir = '/input/'
working_dir = '/working/'

data = pd.read_csv('/content/dataset.csv')
data.columns = ('text','language')
data

# train test split:
x_train, x_test, y_train, y_test = train_test_split(data.text.values, data.language.values, test_size=0.1, random_state=42)
x_train.shape, y_train.shape, x_test.shape, y_test.shape

"""Data Distribution"""

# number of texts for each language:
print(f"Number of languages: {len(data.language.value_counts())}\n")
print(f"Number of data points of individual languages:\n{data.language.value_counts()}")

"""sentence length distribution"""

# sentence length distribution over the entire dataset
sent_lengths = [len(text.split()) for text in data.text.values]
plt.subplots(1,2,figsize=(13,5))
plt.suptitle('Sentence length distribution')
bins=[0,25,50,75,100,125,150,175,200,225,250,275,300,325,350,375,400]
plt.subplot(1,2,1)
sns.histplot(sent_lengths, bins=bins)
plt.subplot(1,2,2)
sns.distplot(sent_lengths, bins=bins)
plt.show()

languages = np.unique(data.language.values).tolist()

plt.subplots(5,5,figsize=(100,100))
i=1
for lang in languages:
    text = data.text.loc[data.language==lang].values
    text = ' '.join(text)
    cloud = WordCloud(max_words=200, width=400, height=400, background_color='white').generate(text)
    plt.subplot(5,5,i)
    plt.imshow(cloud)
    plt.axis("off")
    plt.title(lang,fontsize=100)
    i+=1

# function to clean text
def clean_txt(text):
    text=text.lower()
    text=re.sub(r'[^\w\s]',' ',text)
    text=re.sub(r'[_0-9]',' ',text)
    text=re.sub(r'\s\s+',' ',text)
    return text
# example
txt = 'my (&*(()))name %$#is harsh_priye'
print(clean_txt(txt))

x_train = [clean_txt(text) for text in tqdm(x_train)]
x_test = [clean_txt(text) for text in tqdm(x_test)]

# using Tfidf Vectorizer:
tfidf = TfidfVectorizer()
tfidf.fit(x_train)
x_train_ready = tfidf.transform(x_train)
x_test_ready = tfidf.transform(x_test)
x_train_ready,x_test_ready

enc = LabelEncoder()
enc.fit(y_train)
y_train_ready = enc.transform(y_train)
y_test_ready = enc.transform(y_test)

# storing encoded label hast list as 'labels'
labels = enc.classes_
# display first 10 labels:
labels[:10]

# checking if encoder is working properly
preds = enc.inverse_transform([0,2,5])
preds

nb = MultinomialNB()
nb.fit(x_train_ready,y_train_ready)

from google.colab import drive
drive.mount('/content/drive')

nb_score = nb.score(x_test_ready,y_test_ready)
print(nb_score)

# use pipeline to combine prefitted vectorizer and trained model into one object
model = Pipeline([('vectorizer',tfidf),('nb',nb)])

# save the model:
pickle.dump(model,open('/content/model_v1.pkl','wb'))
# save the encoder
pickle.dump(enc,open('/content/encoder.pkl','wb'))

# function to predict language from text
def predict(text):
    pred = model.predict([clean_txt(text)])
    ans = enc.inverse_transform(pred)
    return ans[0]

predict('my name is harsh'), predict('मेरा नाम हर्ष हे'), predict('mi nombre es harsh'), predict('меня зовут Харш'), predict('mon nom est harsh')